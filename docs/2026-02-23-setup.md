# vibe-local セットアップ編（Windows 11）

> 窓の杜ブログ用記録 | 2026-02-23

## vibe-local とは

ローカルLLM（Ollama + Qwen3）を使って、AIコーディングが完全無料・ネットワーク不要でできるツール。大学の授業やワークショップでの利用を想定した、非営利の研究・教育目的ユーティリティ。

- オリジナル: [`ochyai/vibe-local`](https://github.com/ochyai/vibe-local)（落合陽一氏）
- フォーク: [`aicuai/vibe-local`](https://github.com/aicuai/vibe-local)
- ライセンス: MIT
- 最新バージョン: v0.9.4

> **はかせ曰く:** 「これはなに」「claude codeって外部のLLM使えたんだ」

---

## Windows セットアップの前に：重要な発見

### Windows 版は完全に別アーキテクチャ

セットアップ中に上流リポジトリを `git pull` したところ、Windows 向けのネイティブサポートが追加されていた。

> **はかせ曰く:** 「install.cmdも増えてる？」

確認の結果、Windows 版は macOS/Linux 版と**まったく異なるアーキテクチャ**で動作する：

| | macOS/Linux 版 | **Windows 版** |
|---|---|---|
| エンジン | Claude Code CLI + プロキシ | **vibe-coder.py（独自エージェント）** |
| 必要なもの | Python, Node.js, Claude Code CLI | **Python + Ollama だけ** |
| プロキシ | anthropic-ollama-proxy.py | **不要** |
| Claude Code | 必須 | **不要** |
| Node.js | 必須（Claude Code 用） | **不要** |
| WSL2 | 不要 | **不要** |

**つまり、Windows 版は Python と Ollama さえあれば動く。** Claude Code CLI も Node.js も WSL2 もいらない。学校や会社でインストール権限が制限されている環境でも、Python と Ollama のインストール権限さえあれば使える。

### ソースツリー（v0.9.4）

```
vibe-local/
  install.sh            # macOS/Linux インストーラー
  install.ps1           # Windows インストーラー (803行, Vaporwave UI)
  install.cmd           # install.ps1 のラッパー (.cmd → .ps1)
  vibe-local.sh         # macOS/Linux ランチャー
  vibe-local.ps1        # Windows ランチャー (293行)
  vibe-local.cmd        # vibe-local.ps1 のラッパー
  vibe-coder.py         # スタンドアロン Python コーディングエージェント (5400行)
  anthropic-ollama-proxy.py  # API 変換プロキシ (macOS/Linux 用)
  README.md
  ROADMAP.md
  tests/test_vibe_coder.py
  paper/vibe-coder-technical-report.pdf
  .gitattributes        # *.ps1 → CRLF, *.py → LF
```

### Windows 版の仕組み

```
vibe-local.cmd (ダブルクリックで起動可能)
  ↓ powershell.exe -ExecutionPolicy Bypass -File vibe-local.ps1
vibe-local.ps1 (ランチャー)
  ├ Ollama 起動確認・自動起動
  ├ モデル存在チェック
  ├ 権限チェック（auto-approve / 都度確認）
  └ vibe-coder.py 起動
      ↓ 直接 Ollama API (localhost:11434) に接続
vibe-coder.py (独自コーディングエージェント, 5400行)
  ├ 対話型REPL
  ├ ファイル読み書き・検索
  ├ コマンド実行
  ├ WebSearch (DuckDuckGo)
  └ Ollama と直接通信（プロキシ不要）
```

---

## 環境情報

| 項目 | 値 |
|------|-----|
| OS | Windows 11 Home 10.0.26220 |
| RAM | 32GB（31.7GB 認識） |
| GPU | NVIDIA GeForce RTX 4050 Laptop GPU（VRAM 4GB） |
| GPU (内蔵) | Intel UHD Graphics |
| Python | 3.10.6 / 3.11.5 |
| PowerShell | 5.1.26100.7859（Windows 標準） |
| PowerShell 7 | 7.5.4（pwsh） |
| Ollama | 0.16.3（winget でインストール） |
| winget | v1.12.470 |

### GPU メモリの制約

RTX 4050 Laptop の VRAM は 4GB。モデル選択への影響：

| モデル | サイズ | VRAM 4GB で動くか | 備考 |
|--------|-------|:----------------:|------|
| qwen3-coder:30b | ~19GB | GPU 一部 + CPU | 非常に遅い見込み |
| qwen3:8b | ~5GB | GPU 一部 + CPU | 実用的（メインモデル） |
| qwen3:1.7b | ~1.1GB | GPU で全部載る | 軽量・サイドカー用 |

---

## Windows セットアップ手順

**必要なもの: PowerShell（Windows 標準搭載）と winget だけ。WSL2 不要。**

> **はかせ曰く:** 「もっとログや環境情報など、詳細をそのまま」「セットアップを解説する際はClaude Codeを使わずにWindows PowerShell/CMDだけでセットアップする(WSL2も使わない)、インストール権限だけあればできる(学校などでもいけるかも)、ということを明記して」

### Step 1: Ollama のインストール

PowerShell（またはコマンドプロンプト）で：

```powershell
winget install Ollama.Ollama
```

- バージョン: 0.16.3
- ダウンロードサイズ: 1.17 GB（OllamaSetup.exe）
- インストール後、Ollama は Windows サービスとして自動起動する
- タスクトレイに Ollama アイコンが表示される

**確認:** ブラウザで `http://localhost:11434/` を開くと「Ollama is running」と表示される。

### Step 2: モデルダウンロード

> **はかせ曰く:** 「GUIでqwen3:7bダウンロード終わってます」「ダウンロード終わった」

**方法 A: Ollama GUI から**（推奨）

タスクトレイの Ollama アイコンをクリック → モデル名を入力してダウンロード。

**方法 B: PowerShell から**

```powershell
ollama pull qwen3:8b
ollama pull qwen3:1.7b
```

| モデル | サイズ | 用途 |
|--------|-------|------|
| qwen3:1.7b | 1.3GB (Q4_K_M) | サイドカー（軽量ヘルパー） |
| qwen3:8b | 5.2GB (Q4_K_M) | メインモデル |

### Step 3: install.ps1 でセットアップ

```powershell
# リポジトリのクローン（または ZIP ダウンロード）
git clone https://github.com/ochyai/vibe-local.git
cd vibe-local

# インストーラー実行
powershell -ExecutionPolicy Bypass -File install.ps1
```

または `install.cmd` をダブルクリック。

install.ps1 は以下を自動で行う：
1. ハードウェアスキャン（OS、アーキテクチャ）
2. メモリ検出 → 最適モデル自動選択（32GB → qwen3-coder:30b, 16GB → qwen3:8b, 8GB → qwen3:1.7b）
3. Python / Ollama のインストール確認（未導入なら winget で自動インストール）
4. モデルダウンロード（未ダウンロードなら `ollama pull`）
5. ファイルデプロイ（vibe-coder.py → `%USERPROFILE%\.local\lib\vibe-local\`）
6. 設定ファイル生成（`%USERPROFILE%\.config\vibe-local\config`）
7. PATH 追加（`%USERPROFILE%\.local\bin`）
8. システム診断

Vaporwave テーマの華やかなプログレスバーが表示される。

### Step 4: vibe-local を起動

```powershell
vibe-local              # 対話モード
vibe-local -p "質問"    # ワンショット
vibe-local -Auto        # ネットワーク自動判定（ネットあれば Claude Code、なければローカル）
```

---

## install.ps1 のバグと修正

> **はかせ曰く:** 「いったん Powershell5.1で動く変換を噛まして、動作確認してから本家にPRしてあげて」

### 発見されたバグ（3件）

#### バグ 1: UTF-8 BOM なし（PS 5.1 で致命的）

install.ps1 は UTF-8 で書かれているが BOM（Byte Order Mark）がない。PowerShell 5.1（Windows 10/11 標準）は `.ps1` ファイルを Windows-1252（ANSI）として読むため、日本語文字列が化けてパーサーエラーになる。

**エラー出力:**
```
場所 D:\git.local\vibe-local\install.ps1:51 文字:20
+         tagline = "ネットワーク不要E・ 完E無斁E・ ローカルAIコーチE゙ング"
+                    ~~~~~~~~~~~~~~~~
トークン 'ネットワーク不要E・' を使用できません。
```

51行目の `tagline = "ネットワーク不要 ・ 完全無料 ・ ローカルAIコーディング"` がエンコーディング不一致で壊れ、パーサーが崩壊。以降の行も連鎖的にエラー。

**修正:** UTF-8 BOM（`EF BB BF`）を install.ps1 と vibe-local.ps1 の先頭に追加。

#### バグ 2: Invoke-RestMethod タイムアウト（PS 5.1 で Ollama 検出失敗）

Ollama ヘルスチェックに `Invoke-RestMethod -TimeoutSec 2` を使用。PS 5.1 は最初の .NET HTTP 呼び出しに ~2秒のスタック初期化が必要で、タイムアウト 2秒ではギリギリ間に合わない。

**検証結果:**
```
TimeoutSec=2 : FAIL (2030ms)   ← 初回は必ずタイムアウト
TimeoutSec=3 : OK   (2046ms)   ← ギリギリ成功
TimeoutSec=5 : OK   (10ms)     ← 2回目以降は高速
```

結果、Ollama は実際に動いているのに「起動失敗」と判定 → 30秒待って終了。

**修正:** `Invoke-WebRequest -UseBasicParsing -TimeoutSec 5` に変更。`-UseBasicParsing` は PS 5.1 で IE エンジン依存を回避し、より堅牢。

#### バグ 3: Ollama が PATH にない（PS 5.1 で winget 再インストール）

Ollama GUI インストーラーは `%LOCALAPPDATA%\Programs\Ollama\` に `ollama.exe` を配置するが、このパスが PS 5.1 の PATH に含まれないことがある。

**検証:**
```
PS Version: 5.1.26100.7859
ollama NOT in PATH
FOUND at: C:\Users\shira\AppData\Local\Programs\Ollama\ollama.exe
```

`Get-Command ollama` が失敗 → winget で再インストール → 実行中の Ollama サービスが中断 → ヘルスチェック失敗。

**修正:** 一般的なインストールパス（`%LOCALAPPDATA%\Programs\Ollama\`, `%ProgramFiles%\Ollama\`）をフォールバック検索。

#### その他: Clear-Host エラー（pwsh 7）

PowerShell 7 (pwsh) で `Clear-Host` が `SetValueInvocationException: CursorPosition` エラーを出す。`try { Clear-Host } catch { Write-Host "" }` で回避。

### 修正の検証結果

| テスト | PS 5.1 (修正前) | PS 5.1 (修正後) | pwsh 7.5.4 (修正後) |
|--------|:---:|:---:|:---:|
| 構文パース | NG (パーサーエラー) | OK | OK |
| Step 1-3 (依存関係) | -- | OK | OK |
| Step 4 (Ollama検出) | -- | OK (5s timeout) | OK |
| Step 5-6 (デプロイ) | -- | OK | OK |
| Step 7 (診断) | -- | 全項目 OK | 全項目 OK |
| stderr | 多数 | なし | なし |

### PR 提出と反映

修正を upstream に PR として提出: **[ochyai/vibe-local#5](https://github.com/ochyai/vibe-local/pull/5)**

提出後、kn1cht さん（[@kn1cht](https://github.com/kn1cht)）から補足コメント：

> BOM 追加だけでは HTTP 経由のワンライナーインストール（`Invoke-WebRequest | Invoke-Expression`）で文字化けする場合がある。パイプラインのエンコーディングがシステムデフォルト（Shift-JIS）に影響されるため。`Invoke-RestMethod` を使えば HTTP ヘッダーの `charset=utf-8` を尊重するので文字化けしない。

kn1cht さんは別途 PR #6 で README のインストールコマンドを修正。

| PR | 修正内容 | 対象 |
|---|---|---|
| #5（我々） | BOM追加、Invoke-WebRequest化、Ollama PATH検索 | **ローカルファイル実行** (`git clone` → `powershell -File install.ps1`) |
| #6（kn1cht） | README のワンライナーを `Invoke-RestMethod` に変更 | **HTTP ワンライナーインストール** |

**両方とも数時間で本家に取り込まれた。**

```
1feff61 fix(windows): integrate PR #5 — PS 5.1 compat, BOM, Ollama service fallback
3124dd8 fix(docs): use Invoke-RestMethod for Windows one-liner install (#6)
```

落合氏のコミット `1feff61` には以下のクレジットが記載：

```
Co-Authored-By: Akihiko SHIRAI <kaitas@users.noreply.github.com>
Co-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>
```

PR 提出 → コミュニティからの補足 → 本家マージまで半日。OSS コントリビューションの好例。

---

## 補足: macOS/Linux 版との違い

### macOS/Linux 版の仕組み（参考）

```
Claude Code CLI
  ↓ ANTHROPIC_BASE_URL を localhost:8082 に設定
anthropic-ollama-proxy.py（ローカルプロキシ）
  ├ Anthropic Messages API → OpenAI Chat API にフォーマット変換
  ├ システムプロンプト圧縮（~15K → ~1K）
  ├ ツールフィルタ（20+個 → 9個に削減）
  └ デュアルモデルルーティング（メイン / サイドカー）
  ↓
Ollama (localhost:11434)
```

Claude Code CLI を `ANTHROPIC_BASE_URL` 環境変数で騙す方式。Node.js と Claude Code CLI が必要。

### 通常の Claude Code 環境を壊さないか？

> **はかせ曰く:** 「それが通常のClaude codeの環境を壊さないかどうかを確認してから進めてください」

**結論：壊さない。安全に共存できる。**

| 確認項目 | 結果 | 根拠 |
|---------|------|------|
| 環境変数 | プロセスローカル | `exec claude` の行頭で設定。`export` していない |
| ファイル配置 | 完全に独立 | `~/.local/lib/vibe-local/` と `~/.local/bin/vibe-local` |
| 設定ファイル | 干渉しない | `~/.config/vibe-local/config`（Claude は `~/.claude/`） |
| Ollama | 独立 | `localhost:11434`（Claude は `api.anthropic.com`） |

### Windows 版はさらにシンプル

Windows 版は Claude Code CLI を使わないため、上記の懸念自体が存在しない。Python + Ollama だけで完結する。

---

## アンインストール

```powershell
# vibe-local 関連ファイル
Remove-Item -Recurse "$env:USERPROFILE\.local\lib\vibe-local"
Remove-Item -Recurse "$env:USERPROFILE\.local\bin\vibe-local*"
Remove-Item -Recurse "$env:USERPROFILE\.config\vibe-local"

# Ollama（必要なら）
winget uninstall Ollama.Ollama
```

---

## まとめ

- Windows 11 では **PowerShell/CMD だけ** でセットアップ可能（WSL2 不要、Claude Code 不要、Node.js 不要）
- **インストール権限だけあればOK** — 学校や企業の管理された環境でも導入可能な可能性
- 必要なのは **Python** と **Ollama** の2つだけ
- install.ps1 には PowerShell 5.1 互換性のバグがあったが、修正 PR を提出 → 数時間で本家に取り込まれた
- コミュニティ（kn1cht さん）からワンライナーインストールの補足修正も入り、両方反映
- vibe-coder.py (5400行) が Claude Code CLI を完全に置き換えるスタンドアロンエージェントとして動作
