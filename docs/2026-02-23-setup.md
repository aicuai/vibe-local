# vibe-local セットアップ編（Windows 11）

> 窓の杜ブログ用記録 | 2026-02-23

## vibe-local とは

ローカルLLM（Ollama + Qwen3）を使って、Claude Code のインターフェースでAIコーディングができる無料ツール。ネットワーク不要・完全無料。大学の授業やワークショップでの利用を想定した、非営利の研究・教育目的ユーティリティ。

- オリジナル: `ochyai/vibe-local`（落合陽一氏）
- フォーク: `aicuai/vibe-local`
- ライセンス: MIT

## 仕組み

```
Claude Code CLI
  ↓ ANTHROPIC_BASE_URL を localhost:8082 に設定
anthropic-ollama-proxy.py（ローカルプロキシ）
  ├ Anthropic Messages API → OpenAI Chat API にフォーマット変換
  ├ システムプロンプト圧縮（~15K → ~1K）
  ├ ツールフィルタ（20+個 → 9個に削減）
  └ デュアルモデルルーティング（メイン / サイドカー）
  ↓
Ollama (localhost:11434)
  └ ローカルLLM推論
```

Claude Code は「Anthropic API と通信しているつもり」で動いており、間にプロキシを挟んでリクエストを変換・転送している。

## Claude Code って外部の LLM 使えたの？

### 答え：公式機能ではないが、環境変数のハックで可能

Claude Code CLI は API エンドポイントを `ANTHROPIC_BASE_URL` 環境変数で変更できる。vibe-local はこの仕様を利用し、ローカルプロキシ経由でリクエストを Ollama に転送している。

**vibe-local.sh の該当箇所（391-394行目）：**

```bash
ANTHROPIC_BASE_URL="$PROXY_URL" \
ANTHROPIC_API_KEY="local" \
VIBE_LOCAL_DEBUG="$VIBE_LOCAL_DEBUG" \
exec claude --model "$MODEL" ${PERM_ARGS[@]+"${PERM_ARGS[@]}"} ${EXTRA_ARGS[@]+"${EXTRA_ARGS[@]}"}
```

ポイント：
- `ANTHROPIC_BASE_URL` → ローカルプロキシ (`http://127.0.0.1:8082`) に向ける
- `ANTHROPIC_API_KEY` → `"local"` というダミー値（Ollama に認証は不要だが CLI が空キーを拒否するため）
- プロキシ (`anthropic-ollama-proxy.py`) が Anthropic Messages API → OpenAI Chat API にフォーマット変換

### 通常の Claude Code 環境を壊さないか？

> **はかせ曰く:** 「それが通常のClaude codeの環境を壊さないかどうかを確認してから進めてください」

**結論：壊さない。安全に共存できる。**

| 確認項目 | 結果 | 根拠 |
|---------|------|------|
| 環境変数のスコープ | プロセスローカル | `exec claude` の行頭で設定。`export` でシェル全体に公開していない。別ターミナルの `claude` は影響なし |
| ファイル配置 | 完全に独立 | vibe-local は `~/.local/lib/vibe-local/` と `~/.local/bin/vibe-local` に配置。Claude Code の設定ディレクトリ `~/.claude/` は一切変更しない |
| 設定ファイル | 干渉しない | vibe-local 独自の `~/.config/vibe-local/config` を使用 |
| PATH 変更 | 無害 | `~/.local/bin` を PATH に追加するだけ。`claude` コマンド自体は npm グローバルにインストール済みのものをそのまま使う |
| Ollama プロセス | 独立 | `localhost:11434` で動作。Claude Code 本来の通信先 (`api.anthropic.com`) とは無関係 |
| プロキシプロセス | 自動管理 | 起動時に `python3 anthropic-ollama-proxy.py` を起動し、終了時に `trap cleanup EXIT` で自動停止。PIDファイルで管理 |

```
通常の使い方:
  $ claude                  → api.anthropic.com に接続（いつも通り）

vibe-local 経由:
  $ vibe-local              → localhost:8082（プロキシ）→ localhost:11434（Ollama）
```

同じマシンで両方を使い分けられる。

## 環境情報

| 項目 | 値 |
|------|-----|
| OS | Windows 11 Home 10.0.26220 |
| RAM | 32GB（31.7GB 認識） |
| GPU | NVIDIA GeForce RTX 4050 Laptop GPU（VRAM 4GB） |
| GPU (内蔵) | Intel UHD Graphics |
| Python | 3.10.6 |
| Node.js | v24.11.1 |
| Claude Code CLI | 2.1.50 |
| Ollama | 0.16.3（winget でインストール） |
| winget | v1.12.470 |

> **はかせ曰く:** 「これはなに」「claude codeって外部のLLM使えたんだ」→ 仕組みの解説と安全性確認を依頼。
> 「此処から先のセットアップや実験メモは窓の杜のブログにします」「ダウンロードやセットアップ、推論にかかった時間を都度記録してベンチマーク記事にします」

### 注意点：vibe-local は macOS/Linux 向け

`install.sh` と `vibe-local.sh` は macOS/Linux を前提に書かれている（`uname -s` で Darwin/Linux のみ対応）。
Windows では手動セットアップが必要。今回はそのプロセスを記録する。

### GPU メモリの制約

RTX 4050 Laptop の VRAM は 4GB しかない。モデル選択への影響：

| モデル | サイズ | VRAM 4GB で動くか | 備考 |
|--------|-------|:----------------:|------|
| qwen3-coder:30b | ~19GB | GPU 一部 + CPU | 非常に遅い見込み |
| qwen3:8b | ~5GB | GPU 一部 + CPU | 実用的か要検証 |
| qwen3:1.7b | ~1.1GB | GPU で全部載る | 軽量だが精度低い |

## セットアップ手順（Windows 11）

### Step 1: Ollama のインストール

```
winget install Ollama.Ollama
```

- バージョン: 0.16.3
- ダウンロードサイズ: 1.17 GB（OllamaSetup.exe）
- インストーラーが GUI 起動するため、`ollama pull` はターミナルからの出力キャプチャに問題あり
- Ollama GUI のタスクトレイアイコンから操作する方が Windows では確実

### Step 2: モデルダウンロード

> **はかせ曰く:** 「GUIでqwen3:7bダウンロード終わってます」（Windows では Ollama GUI からのダウンロードが確実）

Ollama GUI からモデル名を入力してダウンロード。

| モデル | サイズ | 備考 |
|--------|-------|------|
| qwen3:1.7b | 1.3GB (Q4_K_M) | サイドカー兼初回テスト用 |
| qwen3:8b | 5.2GB (Q4_K_M) | メインモデル |

### Step 3: プロキシ起動

Windows では `vibe-local.sh`（macOS/Linux 専用）が使えないため、手動でコンポーネントを起動。

```bash
# プロキシ起動（Git Bash から）
cd /d/git.local/vibe-local
OLLAMA_HOST="http://localhost:11434" \
VIBE_LOCAL_MODEL="qwen3:8b" \
VIBE_LOCAL_SIDECAR_MODEL="qwen3:1.7b" \
VIBE_LOCAL_DEBUG="1" \
python anthropic-ollama-proxy.py 8082 &
```

- 起動確認: `curl http://127.0.0.1:8082/` → `{"status": "ok", "proxy": "anthropic-to-ollama"}`

### Step 4: Claude Code をローカル LLM で起動

```bash
# 重要: Claude Code の中から起動する場合は CLAUDECODE 環境変数を解除する必要がある
env -u CLAUDECODE \
ANTHROPIC_BASE_URL="http://127.0.0.1:8082" \
ANTHROPIC_API_KEY="local" \
claude --model qwen3:8b
```

**ハマりポイント：** Claude Code は入れ子起動を禁止している。
`Error: Claude Code cannot be launched inside another Claude Code session.`
→ `env -u CLAUDECODE` で環境変数を解除すれば回避可能。

### アンインストール

vibe-local をアンインストールしたければ、以下を消すだけ：

```
~/.local/bin/vibe-local
~/.local/lib/vibe-local/
~/.config/vibe-local/
```

Ollama は `winget uninstall Ollama.Ollama` で削除。
